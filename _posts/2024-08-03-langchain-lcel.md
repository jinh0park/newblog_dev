---
layout: post
title: "LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‹œì‘í•˜ëŠ” LLMê³¼ Prompt Engineering"
subtitle: "LangChainì„ ì´ìš©í•œ LLM í™œìš© ê°€ì´ë“œ"
date: 2024-08-03 22:43:00 +0900
comments: True
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3;
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

.dataframe tbody tr th:only-of-type {
vertical-align: middle;
}

.dataframe tbody tr th {
vertical-align: top;
}

.dataframe thead th {
text-align: center !important;
padding: 8px;
}

.page\_\_content p {
margin: 0 0 1.3rem !important;
}

.page\_\_content li > p {
margin: 0 0 0.6rem !important;
}

.page\_\_content p > strong {
font-size: 1.0rem !important;
}

  </style>
</head>

**ì£¼ìš”ë‚´ìš©**

-   ğŸ“š LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„¤ì • ë°©ë²•
-   ğŸ”‘ API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ í¸ë¦¬í•œ LLM ì‚¬ìš©
-   ğŸ›  Runnable ê°ì²´ë¥¼ í™œìš©í•œ chain êµ¬ì„±ê³¼ í™œìš©ë²•

# LangChain ê³µë¶€í•˜ê¸°

í…Œë”” ë…¸íŠ¸ë‹˜ì˜ [<ë­ì²´ì¸LangChain ë…¸íŠ¸> - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡·](https://wikidocs.net/book/14314) ë¥¼ ë³´ë©´ì„œ LangChainì„ ê³µë¶€í•˜ê³  ìˆë‹¤. Prompt Engineeringì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ê³µë¶€ë¥¼ í•´ë³´ë‹ˆ ì´ë¥¼ ì§ì ‘ ì ìš©í•˜ì—¬ LLMì„ í™œìš©í•˜ê³  ì‹¶ì–´ì¡Œê³ , ì´ë¥¼ ìœ„í•´ì„œëŠ” LangChainì´ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìµí í•„ìš”ê°€ ìˆë‹¤ê³  ìƒê°í–ˆë‹¤.

í™•ì‹¤íˆ ë§Œë“¤ì–´ì§„ì§€ ì–¼ë§ˆ ì•ˆ ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹ˆë§Œí¼ ì˜¤ë¥˜ë„ ë§ê³ , ìˆ˜ì •ë„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤. êµ¬ê¸€ë§ì„ í•´ë´ë„ ë„ëŒ€ì²´ ì›ì¸ì„ ì•Œ ìˆ˜ê°€ ì—†ì–´ì„œ LangChain Github Repoë¥¼ ë“¤ì–´ê°€ë³´ë‚˜ 13ì‹œê°„ ì „ì— ì˜¤ë¥˜ê°€ ìˆ˜ì •ë˜ì–´ ìˆë‹¤ë“ ê°€... ì—¬í•˜íŠ¼ ê³µë¶€ë„ ì¬ë°Œì§€ë§Œ ì´ëŸ° ì´ˆì°½ê¸° í”„ë¡œì íŠ¸ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ë³´ëŠ” ê²½í—˜ ìì²´ê°€ ì‹ ë¹„ë¡­ë‹¤.

ì´ëŸ° ê°œë°œ ê´€ë ¨ ë¸”ë¡œê·¸ëŠ” Ipython Notebookìœ¼ë¡œ ì‘ì„±í•˜ê³  ì ë‹¹íˆ ë‹¤ë“¬ì€ í›„ (ì´ê²ƒë„ LLMìœ¼ë¡œ ê°€ëŠ¥!) ì˜¬ë¦¬ëŠ”ê²Œ ê½¤ ê´œì°®ì€ ë°©ë²•ì¸ ê²ƒ ê°™ë‹¤. ë§ˆì¹¨ í…Œë”” ë…¸íŠ¸ë‹˜ì´ í•´ë‹¹ ë°©ë²•ë¡ ì— ëŒ€í•œ ê°€ì´ë“œë„ ë§Œë“¤ì–´ì£¼ì…”ì„œ ë³´ê³  ì ìš©í•´ë³´ë ¤ í•œë‹¤.

## ê¸°ë³¸ ì„¸íŒ…

`langchain`ì„ `pip`ë¡œ ì„¤ì¹˜í•´ì£¼ê¸°ë§Œ í•˜ë©´ ëœë‹¤. `langsmith` ì„¤ì •ì€ ë‚˜ì¤‘ì— í•„ìš”ì„±ì„ ëŠë‚„ ë•Œ í•´ë„ ëŠ¦ì§€ ì•Šì„ ê²ƒ ê°™ì•„ì„œ ìƒëµí–ˆë‹¤.

LLMì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ê° ì„œë¹„ìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” API_KEYë¥¼ ë°œê¸‰ ë°›ì•„ì•¼ í•œë‹¤. ì´ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì €ì¥í•´ë‘ë©´ ë”°ë¡œ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•˜ì§€ ì•Šê³  í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

í…Œë””ë…¸íŠ¸ì—ì„œëŠ” `dotenv`ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ë‚˜ëŠ” ê·¸ëƒ¥ ê·€ì°®ì•„ì„œ `.zprofile`ì— (zsh ê¸°ì¤€)ì— ë“±ë¡í–ˆë‹¤. ì£¼ìš” ì„œë¹„ìŠ¤ ë³„ keyëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

-   OpenAI: `OPENAI_API_KEY`

-   Google Gemini: `GOOGLE_API_KEY`

```python
%%capture
%pip install langchain
```

```python
from IPython.display import display, Markdown
```

```python
from langchain_openai import ChatOpenAI

# ê°ì²´ ìƒì„±
llm = ChatOpenAI(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    model_name="gpt-4o-mini",  # ëª¨ë¸ëª…
)

# ì§ˆì˜ë‚´ìš©
question = "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"

# ì§ˆì˜
print(f"[ë‹µë³€]: {llm.invoke(question)}")
```

```
[ë‹µë³€]: content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 16, 'total_tokens': 24}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-4202b29e-4843-4a65-9de9-4591dc0852d3-0' usage_metadata={'input_tokens': 16, 'output_tokens': 8, 'total_tokens': 24}
```

## Chainê³¼ LCEL(LangChain Expression Language)

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì…ë‹ˆê¹Œ?")
```

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    model_name="gpt-4o-mini",  # ëª¨ë¸ëª…
)
```

```python
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()
```

ê¸°ë³¸ì ìœ¼ë¡œ `chain`ì€ `prompt`, `model`, `parser`ì˜ sequentialë¡œ ì´ë£¨ì–´ì§„ë‹¤. ê° ìš”ì†ŒëŠ” ìš©ë„ì— ë”°ë¼ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ë¡œ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í•„ìš”ì— ë”°ë¼ ì ì ˆíˆ ê°€ì ¸ë‹¤ ì“¸ ìˆ˜ ìˆë‹¤.

```python
chain = prompt | llm | output_parser
print(chain)
print(type(chain))
```

```
first=PromptTemplate(input_variables=['country'], template='{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì…ë‹ˆê¹Œ?') middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10e60bc50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10e617590>, model_name='gpt-4o-mini', temperature=0.1, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()
<class 'langchain_core.runnables.base.RunnableSequence'>
```

```python
type(chain)
```

```
langchain_core.runnables.base.RunnableSequence
```

```python
chain.invoke({"country": "ëŒ€í•œë¯¼êµ­"})
```

```
'ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.'
```

íƒ€ì…ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ `chain`ì€ `Runnable`ì˜ sequenceì´ë‹¤. `prompt`, `model`, `parser`ë„ íŠ¹ì •í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ê³  ìˆëŠ” `Runnable`ì˜ í•œ ì¢…ë¥˜ì´ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì•„ë˜ ì½”ë“œì—ì„œ ê° ìš”ì†Œë“¤ë„ ê°œë³„ì ìœ¼ë¡œ `invoke()` methodë¥¼ í˜¸ì¶œ ê°€ëŠ¥í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
print(prompt.invoke({"country": "ëŒ€í•œë¯¼êµ­"}))
output = llm.invoke("ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?")
print(output)
print(output_parser.invoke(output))
```

```
text='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì…ë‹ˆê¹Œ?'
content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 13, 'total_tokens': 21}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-9c6f9ff5-699c-40ef-ac25-28cc70f33f5a-0' usage_metadata={'input_tokens': 13, 'output_tokens': 8, 'total_tokens': 21}
ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.
```

## Runnable

chainì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì„±í•˜ê¸° ìœ„í•´ ì—°ê²°ê³ ë¦¬ ì—­í• ì„ í•˜ëŠ” `Runnable` ê°ì²´ë“¤ì´ ìˆë‹¤.

-   `RunnablePassThrough`

-   `RunnableParalle`

-   `RunnableLambda`

### RunnablePassThrough

ì•„ë˜ ë‘ ì½”ë“œëŠ” ì™„ì „íˆ ë™ì¼í•˜ë‹¤. ì¦‰, `dict`ë¥¼ chainì— í¬í•¨ì‹œí‚¤ê³ ì í•  ë•Œ `RunnablePassThrough`ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤.

```python
from langchain_core.runnables import RunnablePassthrough

print(
    prompt.invoke({"country":"ëŒ€í•œë¯¼êµ­"}) ==
    ({"country": RunnablePassthrough()} | prompt).invoke("ëŒ€í•œë¯¼êµ­")
)

prompt.invoke({"country":"ëŒ€í•œë¯¼êµ­"})
```

```
True
```

```
StringPromptValue(text='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?')
```

`assign()` methodë¥¼ ì´ìš©í•˜ì—¬, inputìœ¼ë¡œ ë°›ì€ `dict`ì— ìƒˆë¡œìš´ í‚¤-ë°¸ë¥˜ ìŒì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.

```python
# ì…ë ¥ í‚¤: num, í• ë‹¹(assign) í‚¤: new_num
(RunnablePassthrough.assign(address=lambda x: x["country"] + " " +  x["city"])).invoke({"country": "ëŒ€í•œë¯¼êµ­", "city": "ì„œìš¸"})
```

```
{'country': 'ëŒ€í•œë¯¼êµ­', 'city': 'ì„œìš¸', 'address': 'ëŒ€í•œë¯¼êµ­ ì„œìš¸'}
```

### RunnableParallel

`RunnableParallel`ì€ inputì„ ë°›ìœ¼ë©´, ì´ë¥¼ ì§€ì •í•œ parameter ìˆ˜ ë§Œí¼ì˜ `Runnable`(ë˜ëŠ” callable, dict)ì— ì „ë‹¬í•˜ì—¬ ë¶„ê¸°ì ì„ í˜•ì„±í•˜ëŠ” ê°ì²´ì´ë‹¤.

```python
from langchain_core.runnables import RunnableParallel

RunnableParallel(out1=lambda x: x, out2=lambda x:x+1, out3=lambda x:x+2).invoke(1)
```

```
{'out1': 1, 'out2': 2, 'out3': 3}
```

`RunnableParallel`ì„ ì´ìš©í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ chainì„ ë³‘ë ¬ì ìœ¼ë¡œ ì—°ê²°í•  ìˆ˜ ìˆë‹¤.

`RunnableParallel`ë¡œ chainì„ ì—°ê²°í•  ê²½ìš° ë³‘ë ¬ì²˜ë¦¬ë˜ì–´ ì‹¤í–‰ì‹œê°„ë©´ì—ì„œ ì´ë“ì„ ë³¼ ìˆ˜ ìˆë‹¤.

```python
llm = ChatOpenAI(
    temperature=0.1,  # ì°½ì˜ì„± (0.0 ~ 2.0)
    model_name="gpt-4o",  # ëª¨ë¸ëª…
)

chain1 = PromptTemplate.from_template("{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì…ë‹ˆê¹Œ?") | llm | StrOutputParser()
chain2 = PromptTemplate.from_template("{country}ì˜ ì¸êµ¬ëŠ” ëª‡ ëª…ì…ë‹ˆê¹Œ?") | llm | StrOutputParser()
chain3 = PromptTemplate.from_template("{country}ì˜ ë©´ì ì€ ì–¼ë§ˆì…ë‹ˆê¹Œ?") | llm | StrOutputParser()

combined_chain = ({"country": RunnablePassthrough()}
                  | RunnableParallel(capital=chain1, population=chain2, area=chain3))
```

```python
combined_chain.invoke("ëŒ€í•œë¯¼êµ­")
```

```
{'capital': 'ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.',
 'population': '2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ëŠ” ì•½ 5,100ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì§€ì†ì ìœ¼ë¡œ ë³€ë™í•˜ë¯€ë¡œ, ìµœì‹  í†µê³„ëŠ” ëŒ€í•œë¯¼êµ­ í†µê³„ì²­ì´ë‚˜ ê´€ë ¨ ê¸°ê´€ì˜ ê³µì‹ ìë£Œë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.',
 'area': 'ëŒ€í•œë¯¼êµ­ì˜ ë©´ì ì€ ì•½ 100,210 í‰ë°©í‚¬ë¡œë¯¸í„°ì…ë‹ˆë‹¤. ì´ëŠ” í•œë°˜ë„ì˜ ë‚¨ìª½ ë¶€ë¶„ì— í•´ë‹¹í•˜ë©°, ë¶í•œê³¼ í•¨ê»˜ í•œë°˜ë„ë¥¼ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤.'}
```

### RunnableLambda

`RunnableLambda`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ë§µí•‘í•  ìˆ˜ ìˆë‹¤.

ì…ë ¥ ë³€ìˆ˜ê°€ í•„ìš”í•˜ì§€ ì•Šì€ í•¨ìˆ˜ì˜ ê²½ìš°

```python
from langchain_core.runnables import RunnableLambda
from datetime import datetime


def get_today(a):
    return datetime.today().strftime("%b-%d")

print(get_today(None))

RunnableLambda(get_today).invoke("")
```

```
Aug-04
```

```
'Aug-04'
```

ì…ë ¥ ë³€ìˆ˜ê°€ 1ê°œì¸ í•¨ìˆ˜ì˜ ê²½ìš°

```python
def get_text_length(text):
    return len(text)

print(get_text_length("pizza"))

RunnableLambda(get_text_length).invoke("pizza")
```

```
5
```

```
5
```

ì…ë ¥ ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, ì¸ìˆ˜ë¥¼ ë”•ì…”ë„ˆë¦¬ í•˜ë‚˜ë¡œ ë°›ì•„ì„œ ì› í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” í˜•íƒœì˜ wrapping í•¨ìˆ˜ë¥¼ ì¬ì •ì˜í•˜ì—¬ í™œìš©í•´ì•¼ í•œë‹¤.

```python
def concat_text(text1, text2):
    return text1 + "-" + text2

def _concat_text(_dict):
    return concat_text(_dict["text1"], _dict["text2"])

print(concat_text("potato", "pizza"))

RunnableLambda(_concat_text).invoke({"text1":"potato", "text2":"pizza"})
```

```
potato-pizza
```

```
'potato-pizza'
```

êµ³ì´ í•¨ìˆ˜ë¥¼ ì¬ì •ì˜í•˜ì§€ ì•Šê³  unpacking ì—°ì‚°ìì™€ lambdaë¥¼ í™œìš©í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.

```python
def concat_text(text1, text2):
    return text1 + "-" + text2

RunnableLambda(lambda _: concat_text(**_)).invoke({"text1":"potato", "text2":"pizza"})
```

```
'potato-pizza'
```

ì•„ë˜ì™€ ê°™ì´ í™œìš©í•  ìˆ˜ ìˆë‹¤.

```python
chain = (
    {"today": RunnableLambda(get_today)}
    | PromptTemplate.from_template("{today}ì— ê°€ê¹Œìš´ ê¸°ë…ì¼ì„ ë‚˜ì—´í•´ì¤˜")
    | llm
    | StrOutputParser()
)

display(Markdown(chain.invoke("")))
```

```
<IPython.core.display.Markdown object>
```
