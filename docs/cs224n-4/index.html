<!DOCTYPE html> <!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--> <!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--> <!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--> <!--[if gt IE 8]><!--> <html class="no-js"><!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <title>강의 요약 - CS224n: Natural Language Processing with Deep Learning (4) &#8211; JINH-ZERO-PARK</title> <meta name="description" content="Welcome."> <meta name="keywords" content=""> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="/assets/img/logo.png"> <meta name="twitter:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning (4)"> <meta name="twitter:description" content="I. Contents"> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning (4)"> <meta property="og:description" content="I. Contents"> <meta property="og:url" content="/cs224n-4/"> <meta property="og:site_name" content="JINH-ZERO-PARK"> <meta property="og:image" content="/assets/img/logo.png"> <link rel="canonical" href="/cs224n-4/"> <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="JINH-ZERO-PARK Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="/assets/css/main.css"> <!-- JS --> <script src="/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="/favicon.png" /> <link rel="shortcut icon" href="/favicon.ico" /> <!-- Background Image --> <style type="text/css">body {background-image:url(/assets/img/placeholder-big.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="/assets/img/logo.png" alt="JINH-ZERO-PARK photo" class="author-photo"> <h4>JINH-ZERO-PARK</h4> <p>Welcome.</p> </li> <li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="mailto:jinh0park@naver.com" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-envelope-square"></i> Email</a> </li> <li> <a href="http://instagram.com/jinh0park" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-instagram"></i> Instagram</a> </li> <li> <a href="http://github.com/jinh0park" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> </ul><!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="/posts/">All Posts</a></li> <li><a href="/tags/">All Tags</a></li> </ul> </li> <li><a href="/projects/" >Projects</a></li> </ul><!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>강의 요약 - CS224n: Natural Language Processing with Deep Learning (4)</h1> <h4>30 Aug 2022</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~5 minutes </p><!-- /.entry-reading-time --> <a class="btn zoombtn" href="/posts/"> <i class="fa fa-chevron-left"></i> </a> </div> <h1 id="i-contents">I. Contents</h1> <ul> <li>Machine translation</li> <li>Seq2seq <ul> <li>Neural Machine Translation</li> <li>Training a Neural Machine Translation System</li> <li>Multi-layer RNNs</li> <li>Decoding varieties</li> <li>Evaluating Maching Translation</li> </ul> </li> <li>Attention <ul> <li>Seq2seq: the bottleneck problem</li> <li>Attention</li> </ul> </li> </ul> <h1 id="ii-machine-translation">II. Machine Translation</h1> <p>Machine Translation is the task of translating a sentence \(x\) from one language (<strong>the source language</strong>) to a sentence \(y\) in another language (<strong>the target language</strong>).</p> <h2 id="1990s-2010s-statistical-machine-translation">1990s-2010s: Statistical Machine Translation</h2> <p><img src="/img/posts/cs224n/15.png" alt="" /></p> <h3 id="alignment">Alignment</h3> <ul> <li>Q. How to learn translation model \(P(x\mid y)\)? <ul> <li>First, need large amount of parallel data e.g. <em>The Rosetta Stone</em></li> <li>Break it down further: Introduce latent \(a\) variable into the model: \(P(x, a\mid y)\) where \(a\) is the <strong>alignment</strong>, i.e. word-level correspondence between source sentence \(x\) and target sentence \(y\)</li> <li>Alignments are <strong>latent variables</strong>: They aren’t explicitly specified in the data!</li> </ul> </li> </ul> <p>언어마다 문법이나 단어 체계가 다르기 때문에, 번역을 하기 위해서는 source sentence와 target sentence 의 단어가 각각 어떻게 대응되는지 파악해야하며, 이를 alignment라고 한다.</p> <p>Alignment는 one-to-one, many-to-one, one-to-many, many-to-many 등 복잡하게 구성되며, dependency parsing에서의 arc처럼 명시적으로 특정되지 않고 SMT에 내장되므로 <strong>latent variable</strong>이라고 부른다.</p> <h3 id="decoding-for-smt">Decoding for SMT</h3> <p>Enumerating every possible \(y\) and calculate the probability is too expensive.</p> <p>Answer : Impose strong independence assumptions in model, use dynamic programming for globally optimal solutions</p> <p><img src="/img/posts/cs224n/16.png" alt="" /></p> <h3 id="conclusion">Conclusion</h3> <p>The best systems of SMT were “extremely complex”</p> <h1 id="iii-seq2seq">III. Seq2Seq</h1> <h2 id="1-neural-machine-translation">1. Neural Machine translation</h2> <p><strong>Neural Machine Translation (NMT)</strong> is a way to do Machine Translation with a <em>single end-to-end neural network</em></p> <p>The neural network architecture is called a <strong>sequence-to-sequence</strong> model (a.k.a. <strong>seq2seq</strong>) and it involves <strong>two RNNs</strong></p> <p><img src="/img/posts/cs224n/17.png" alt="" /></p> <ul> <li>seq2seq is useful for <strong>more than just MT</strong> <ul> <li>Summarization</li> <li>Dialogue</li> <li>Parsing</li> <li>Code generation</li> </ul> </li> <li>seq2seq model is an example of a <strong>Conditional Language Model</strong></li> </ul> <h2 id="2-training-a-neural-machine-translation-system">2. Training a Neural Machine Translation System</h2> <p>\[J(\theta) = \frac {1}{T} \sum_{t=1}^T J_t\] (\(J_t\) is negative lof prob of the word)</p> <p>seqseq is optimized as a <strong>single system</strong>, Backpropagation operates “end-to-end”</p> <h2 id="3-multi-layer-rnns">3. Multi-layer RNNs</h2> <ul> <li>High-performing RNNs are usually multi-layer : 2 to 4 layers!</li> <li>Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g. 8 layers)</li> <li>Transformer-based networks (e.g. BERT) are usually deeper, like 12 or 24 layers.</li> </ul> <h2 id="4-decoding-varieties">4. Decoding varieties</h2> <h3 id="greedy-decoding">Greedy Decoding</h3> <ul> <li>Take most probable word on each step</li> <li><strong>Greedy decoding has no way to undo decisions</strong></li> </ul> <h3 id="exhaustive-search-decoding">Exhaustive search Decoding</h3> <ul> <li>Ideally : We could tru computing all possible sequences \(y\) and find \(y\) that maximizes : \[P(y\mid x)=\prod^T_{t=1}P(y_t\mid y_1,\cdots ,y_{t-1},x)\]</li> <li><strong>This \(O(V^T)\) complexity is far too expensive!!!</strong></li> </ul> <h3 id="beam-search-decoding">Beam search Decoding</h3> <ul> <li>Core idea : On each step of decoder, keep track of the <em>k most probable partial translations</em> (which we call <strong>hypotheses</strong>)</li> <li>Beam search is not guaranteed to find optimal solution, but much more efficient than exhaustive search.</li> </ul> <h3 id="advantages-and-disadvantages-of-nmt">Advantages and Disadvantages of NMT</h3> <h4 id="advantages">Advantages</h4> <ul> <li>Better performance</li> <li>A single neural network to be optimized end-to-end <ul> <li>No subcomponents to be individually optimized</li> </ul> </li> <li>Requires much less human engineering effort <ul> <li>No feature engineering</li> <li>Same method for all language pairs</li> </ul> </li> </ul> <h4 id="disadvantages">Disadvantages</h4> <p>Compared to SMT :</p> <ul> <li>Less interpretable</li> <li>Difficult to control (e.g. can’t easily specify rules or guidelines for translation)</li> </ul> <h2 id="5-evaluating-maching-translation">5. Evaluating Maching Translation</h2> <p><strong>BLEU</strong> (Bilingual Evaluation Understudy)</p> <p>BLEU compares the machine-written translation to one or several human-written translation(s), and computes a similarity score based on :</p> <ul> <li>n-gram precision</li> <li>Plus a penalty for too-short system translations</li> </ul> <p><a href="http://incredible.ai/nlp/2020/02/29/BLEU/">Learn More - Incredible.AI : BLEU</a></p> <ul> <li>BLEU is useful, but imperfect</li> </ul> <h1 id="iv-attention">IV. Attention</h1> <h2 id="1-seq2seq-the-bottleneck-problem">1. Seq2seq: the bottleneck problem</h2> <p>The last hidden state of encoder which is fed to decoder <strong>needs to capture all information</strong> about toe source sentence. ☞ “Information Bottleneck!”</p> <h2 id="2-attention">2. Attention</h2> <h3 id="overview">Overview</h3> <p><strong>Attention</strong> provides a solution to the bottleneck problem</p> <p>Core idea: on each step of the decoder, user direct connection to the encoder to focus on a particular part of the source sequence</p> <p>실제로 사람이 번역을 할 때에도, source sentence를 읽고 곧바로 target sentence를 써내려가기보다는 target sentence를 작성하면서 source sentence를 다시 읽어보기도 하고, 계속 시선이 왔다갔다 한다. 이러한 컨셉을 direct connection으로 구현한 것이 Attention이다.</p> <p><img src="/img/posts/cs224n/attention.gif" alt="" /></p> <h3 id="attention-in-equations">Attention in equations</h3> <p>CS224n Assignment 4 Handout : Attention with Bidirectional LSTMs</p> <p><img src="/img/posts/cs224n/19.PNG" alt="" /> <img src="/img/posts/cs224n/20.PNG" alt="" /></p> <h3 id="attention-is-great">Attention is great!</h3> <ul> <li>Attention significantly improves NMP performance</li> <li>Attention solves the bottleneck problem</li> <li>Attention helps with vanishing gradient problem</li> <li>Attention provides some interpretability <ul> <li>By inspecting attention distribution, we can get (soft) <strong>alignment for free!</strong></li> <li>The network just learned alignment by itself</li> </ul> </li> </ul> <h3 id="attention-is-a-general-deep-learning-technique">Attention is a general Deep Learning technique</h3> <ul> <li>You can use attention in “many architectures” (not just seq2seq) and “many tasks” (not just MT)</li> <li>More general definition of Attention</li> </ul> <blockquote> <p>Given a set of vector <strong>values</strong>, and a vector <strong>query</strong>, attention is a technique to compute a weighted sum of the values, dependent on the query</p> </blockquote> <ul> <li>We somtimes say that the “query attends to the values.” <ul> <li>e.g. in the seq2seq + attention model, each <strong>decoder hidden state (query)</strong> attends to all the <strong>encoder hiddent states (values)</strong>.</li> </ul> </li> </ul> <p><a href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms">StackExchange : What exactly are keys, queries, and values in attention mechanisms?</a></p> <p>Attention에서 말하는 key, query, 그리고 value가 무엇인지 선뜻 이해되기 어렵다. 위 StackExchange 답변에 자세히 설명되어있으며, 이해한 바를 옮기자면 다음과 같다.</p> <p>일단, key, query, value는 Retrieval System에서 통용되는 개념이다. 가령 유튜브에서 영상을 검색한다고 하면, 각각 아래와 같은 의미를 가진다.</p> <blockquote> <p>The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).</p> </blockquote> <p>그리고 Attention model에서 어떤 값이 각각 key, query, value에 대응되는지 알아야 그 다음을 이해할 수 있다.</p> <p><img src="/img/posts/cs224n/21.PNG" alt="" /></p> <ul> <li>key는 첫번째 식에서의 \(h_1,\cdots ,h_N\)</li> <li>query는 첫번째 식에서의 \(s_t\)</li> <li>value는 세번째 식에서의 \(h_i\)에 대응된다.</li> </ul> <p>우선, Attention이란 무엇인가? 바로 각 step마다 source sentence 중 어떤 부분에 “attention”을 두고 단어를 생성(if MT)할지 결정하는 것이다. \(\alpha^t\)를 가중치로 value들의 weighted sum을 계산한다는 말은, 큰 가중치를 갖는 부분에 큰 “attention”을 둔다는 것을 의미한다.</p> <p>\(\alpha^t\)를 softmax가 아닌 one-hot vector라고 생각하면 더욱 명확해진다. one-hot vector라면 encoder가 생성한 수개의 hidden state (value) 중 단 하나만 골라서 이 부분에’만’ attention을 두고 decoding을 하게되는 것이다. 더 나아가, one-hot vector의 마지막 원소가 1이라면 이는 Simple seq2seq과 동치임을 알 수 있다.</p> <p>그렇다면 다시 key, query, value의 개념으로 돌아가서, 왜 key와 query의 내적값으로 가중치 \(\alpha^t\)를 구하는걸까? 위의 유튜브 검색 비유를 통해 보자면, key를 query에 mapping하여 그 값을 토대로 value를 산출한다. Attention에서는 key와 value를 mapping하는 방법으로 벡터간의 내적을 채택하는 것이다.</p> <p>Decoder의 매 Step마다 query (\(s_t\))는 매번 바뀌고, key (\(h_1,\cdots ,h_N\))는 고정된 값임을 되짚어본다면 왜 key와 query인지 이해할 수 있을 것이다.</p> <h3 id="attention-variants">Attention variants</h3> <p>\(\alpha^t = softmax(e^t)\) 에서 \(e^t\) 를 계산하는 방법으로 dot-product만 논했으나, 여러가지 방법이 더 있다.</p> <ul> <li>Basic dott-product attention: \[e_i=s^Th_i\in \mathbb{R} \]</li> <li>Multiplicative attention \[e_i=s^TWh_i\in \mathbb{R} \]</li> <li>Reduced rank multiplicative attention \[e_i=s^T(U^TV)h_i=(Us)^T(Vh_i)\in \mathbb{R} \]</li> <li>Additive attention \[e_i=v^Ttanh(W_1h_i+W_2s) \in \mathbb{R} \]</li> </ul> <div class="entry-meta"> <br> <hr> <span class="entry-tags"></span> <!-- <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=/cs224n-4/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Share</span> </a> <a href="https://twitter.com/intent/tweet?text=/cs224n-4/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=/cs224n-4/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> --> <div style="clear:both"></div> </div> </div> </div> <section id="disqus_thread" class="animated fadeInUp"></section><!-- /#disqus_thread --> </header> <!-- JS --> <script src="/assets/js/jquery-1.12.0.min.js"></script> <script src="/assets/js/jquery.dlmenu.min.js"></script> <script src="/assets/js/jquery.goup.min.js"></script> <script src="/assets/js/jquery.magnific-popup.min.js"></script> <script src="/assets/js/jquery.fitvid.min.js"></script> <script src="/assets/js/scripts.js"></script> <script type="text/javascript"> var disqus_shortname = 'jinh0park'; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
