<!DOCTYPE html> <!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--> <!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--> <!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--> <!--[if gt IE 8]><!--> <html class="no-js"><!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <title>강의 요약 - CS224n: Natural Language Processing with Deep Learning (1) &#8211; JINH-ZERO-PARK</title> <meta name="description" content="Welcome."> <meta name="keywords" content=""> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="/assets/img/logo.png"> <meta name="twitter:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning (1)"> <meta name="twitter:description" content="I. Contents"> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning (1)"> <meta property="og:description" content="I. Contents"> <meta property="og:url" content="/cs224n-1/"> <meta property="og:site_name" content="JINH-ZERO-PARK"> <meta property="og:image" content="/assets/img/logo.png"> <link rel="canonical" href="/cs224n-1/"> <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="JINH-ZERO-PARK Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="/assets/css/main.css"> <!-- JS --> <script src="/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="/favicon.png" /> <link rel="shortcut icon" href="/favicon.ico" /> <!-- Background Image --> <style type="text/css">body {background-image:url(/assets/img/placeholder-big.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="/assets/img/logo.png" alt="JINH-ZERO-PARK photo" class="author-photo"> <h4>JINH-ZERO-PARK</h4> <p>Welcome.</p> </li> <li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="mailto:jinh0park@naver.com" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-envelope-square"></i> Email</a> </li> <li> <a href="http://instagram.com/jinh0park" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-instagram"></i> Instagram</a> </li> <li> <a href="http://github.com/jinh0park" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> </ul><!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="/posts/">All Posts</a></li> <li><a href="/tags/">All Tags</a></li> </ul> </li> <li><a href="/projects/" >Projects</a></li> </ul><!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>강의 요약 - CS224n: Natural Language Processing with Deep Learning (1)</h1> <h4>27 Aug 2022</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~4 minutes </p><!-- /.entry-reading-time --> <a class="btn zoombtn" href="/posts/"> <i class="fa fa-chevron-left"></i> </a> </div> <h2 id="i-contents">I. Contents</h2> <ul> <li>Human language and word meaning <ul> <li>How do we represent the meaning of a word?</li> <li>Word2Vec</li> <li>GloVe</li> </ul> </li> <li>Summary</li> </ul> <h2 id="ii-human-language-and-word-meaning">II. Human language and word meaning</h2> <h3 id="1-how-do-we-represent-the-meaning-of-a-word">1. How do we represent the meaning of a word?</h3> <h4 id="1-definition--meaning">(1) Definition : <strong>meaning</strong></h4> <p>Webster 영영사전 : the idea that is represented by a word, phrase, etc.</p> <p>표준국어대사전(“의미”) : 말이나 글의 뜻</p> <h4 id="2-commonest-linguistic-way-of-thinking-of-meaning-">(2) Commonest linguistic way of thinking of meaning :</h4> <p>\[signifier(symbol) ↔ signified(idea or thing)\]</p> <h4 id="3-common-nlp-solution">(3) Common NLP solution</h4> <p><em>WordNet</em>, a thesaurus containing lists of synonym sets and hypernyms</p> <p>동의어, 유의어(synonyms) 또는 상의어(hypernyms)로 이루어진 사전을 통해 의미를 정의할 수 있다.</p> <p>※ WordNet의 문제점</p> <ul> <li>뉘앙스를 담을 수 없다.</li> <li>단어의 새로운 의미를 담을 수 없다.</li> <li>동의어 등을 판단하는 기준이 주관적이다.</li> <li>제작하는데 인력이 많이 소모된다.</li> <li><strong>Word Similarity를 계산할 수 없다.</strong></li> </ul> <h4 id="4-representing-words-as-discrete-symbols">(4) Representing words as discrete symbols</h4> <p>“One-hot vector” representing : \[motel = [0, 0, 0, 0, 0, 1, 0] \ hotel = [0, 0, 0, 1, 0, 0, 0]\]</p> <p>※ One-hot vector representing의 문제점 모든 vector들이 orthogonal하기 때문에, similarity를 계산할 수 없다.</p> <h4 id="5-representing-words-by-their-context">(5) Representing words by their context</h4> <p><strong>Distributional semantics</strong> : A word’s meaning is given by the words that frequency appear close-by. When a word \(w\) appears in a textm its context is the set of words that appears nearby.</p> <p>특정 단어의 의미는, 글에서 그 특정 단어 주변에 어떤 단어들(=context)이 주로 오는지에 따라 파악할 수 있다. 위 아이디어를 이용하여, 단어를 Vectorize 할 수 있다.</p> <p>Note : Word vectors are also called word embeddings or word representations, They are a <strong>distributed representation</strong>.</p> <h4 id="6-conclusion">(6) Conclusion</h4> <p>NLP에서 단어를 표현하는 방식으로 WordNet, discrete representations, distributed representation 등이 있다. 이때 NLP에서 “유용한” 방식으로 단어를 표현하기 위해서는, 단어 간 유사도(Similarity)를 계산할 수 있도록 distributed representation이 가장 적절한 해법이다. Distributional semantics에서 착안하여, 단어들을 실수 벡터로 표현하는 방법으로 Word2Vec을 알아본다.</p> <h3 id="2-word2vec">2. Word2Vec</h3> <h4 id="1-what-is-word2vec">(1) What is Word2Vec?</h4> <p><strong>Word2Vec</strong> (Mikolov et al. 2013, <a href="http://arxiv.org/pdf/1301.3781.pdf">PDF</a>) is a framework for learning word vectors.</p> <p><img src="/img/posts/cs224n/1.PNG" alt="" width="60%" class="center-image" /></p> <p>\[Likelihood = L(\theta) = \prod_{t=1}^{T} \prod_{-m \le j\le m, j\ne0} P(w_{t+j}|w_t;\theta)\] \[objective function = J(\theta) = -\frac{1}{T}log(L(\theta))\]</p> <p>특정 단어에 대하여, 주변(Window)에 다른 단어들이 나타날 확률의 곱 (첫번째 \(\prod\))을 모든 단어마다 계산해 곱해주면 (두번째 \(\prod\)), 이를 Likelihood라고 하며, -Log를 취하여 objective function을 최소화 함으로써 word representation을 찾을 수 있다.</p> <h5 id="likelihood란">Likelihood란?</h5> <p>관측 데이터 \(x\)가 있을 때, 어떤 분포 \(\theta\)를 주고, 그 분포에서 데이터가 나왔울 확률을 말한다. \[L(\theta|x)=P_{\theta}(X=x)=\prod_{k=1}^n P(x_k|\theta)\] Word2Vec에서 \(\theta\)란 “to be defined in terms of our vectors”, 즉 optimze할 word vector들의 값을 의미하며(이에 따라 분포가 정해진다), 위 Likelihood의 정의로부터 본다면, \(w_t\)마다 likelihood를 구한 후 T번 곱하는 것이라고 볼 수 있겠다.</p> <h4 id="2-how-to-calculate-the-probability">(2) How to calculate the probability?</h4> <p>\[P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}\] This is an example of the <strong>softmax function</strong>. The softmax fuction maps arbitrary values \(x_i\) to a probability distribution \(p_i\) (“max” because amplifies probability to smaller \(x_i\), “soft” because still assigns some probability to smaller \(x_i\))</p> <h4 id="3-optimization">(3) Optimization</h4> <p>Gradient descent, chain rule, SGD … (생략)</p> <h4 id="4-word2vec--more-details">(4) Word2Vec : More details</h4> <ol> <li>Two model variants <ul> <li>Skip-grams (SG) ☞ 위에서 한 내용</li> <li>Continuous Bag of Words (CBOW)</li> </ul> </li> <li>Additional efficiency in Training <ul> <li>Naive softmax</li> <li>Negative samlping</li> </ul> </li> </ol> <h4 id="5-the-skip-gram-model-with-negative-sampling-pdf">(5) The skip-gram model with negative sampling <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">PDF</a></h4> <p>Main idea : train binary logistic regressions for a true pair(center word and a word in its context window) versus several noise pairs (the center word paired with a random word)</p> <p>Likelihood를 계산하는 식 중, softmax는 computationally expensive하므로(vocabulary에 있는 모든 단어에 대해 내적 필요), 이를 Negative-sampling을 이용한 방식으로 대체한다.</p> <h3 id="3-glove">3. GloVe</h3> <h4 id="1-co-occurrence-matrix-k">(1) Co-occurrence matrix K</h4> <p><img src="/img/posts/cs224n/2.PNG" alt="" width="60%" class="center-image" /></p> <ul> <li>Window length 1 (most common : 5-10)</li> <li>[“I like deep learning”, “I like NLP”, “I enjoy flying”]</li> </ul> <p>Co-occurrence matrix는 V * V 크기의 행렬이고, sparsity issue가 있으므로 차원을 낮추는 과정이 필요하며, 대표적으로 SVD(Singular Value Decomposition)이 있다.</p> <p>이때, raw counts에 SVD를 바로 적용하는 것은 잘 작동하지 않고, the, he, has와 같이 과도하게 자주 등장하는 단어들(function words)의 count를 clipping하거나, count에 log를 취하거나, function words를 무시하는 방법 등을 추가로 요한다.</p> <h4 id="2-glove">(2) GloVe</h4> <p><a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a></p> <p>Q: How to we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?</p> <p>A: Log-bilinear model (아래는 with vector differences) \[w_i\cdot w_j=logP(i|j) \ w_x\cdot (w_a-w_b) = log {\frac{P(x|a)}{P(x|b)}}\]</p> <table> <tbody> <tr> <td>\(w_i\cdot w_j=logP(i</td> <td>j)\) 가 되도록 \(w\)를 optimize하는 것이 목표이다. \(P\)는 co-occurrence matrix로 부터 계산됨.</td> </tr> </tbody> </table> <p>\[J = \sum^V_{i,j=1}f(X_{ij})(w_i^T\tilde w_j+b_i+\tilde b_j - logX_{ij})^2\]</p> <p>\(f\) -&gt; clipping function fot function words</p> <ul> <li>Fast training</li> <li>Scalable to huge corpora</li> <li>Good performance even with small corpus and small vectors</li> </ul> <h4 id="3-evaluation-of-word-vectors">(3) Evaluation of word vectors</h4> <p>Related to general evaluation in NLP : Intrinsic vs. extrinsic</p> <h5 id="intrinsic-word-vector-evaluation">Intrinsic word vector evaluation</h5> <ol> <li>Word vector analogies (ex. man : woman = king : ???, ??? = queen)</li> <li>Word vector distances and their correlation with human judgements</li> </ol> <h5 id="extrinsic-word-vector-evaluation">Extrinsic word vector evaluation</h5> <ol> <li>Named entity recognition</li> </ol> <h4 id="4-word-senses-and-word-sense-ambiguity">(4) Word senses and word sense ambiguity</h4> <p>Most words have lots of meanings!</p> <p><img src="/img/posts/cs224n/3.PNG" alt="" width="60%" class="center-image" /></p> <ol> <li><a href="http://www.aclweb.org/anthology/Q15-1016">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a> Huang et al. 2012</li> <li><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a> Arora et al. 2018</li> </ol> <h1 id="iii-summary">III. Summary</h1> <p>Human language를 컴퓨터가 이해할 수 있도록 표현하기 위해서는 “word vector”를 설계해야한다. 이를 위한 시도들로 Word2Vec, GloVe를 알아보았다. Word2Vec은 SG 또는 CBOW로 구현할 수 있고, gradient descent를 효율적으로 하기 위해 negative sampling을 도입할 수 있다. GloVe는 co-occurrence matrix를 이용하여 word vector를 구한다.</p> <p>Word vector를 만들었으면 이를 평가할 수 있어야 한다. NLP에서 평가 지표로 Intrinsic evaluation 또는 Extrinsic evaluation이 있다.</p> <div class="entry-meta"> <br> <hr> <span class="entry-tags"></span> <!-- <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=/cs224n-1/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Share</span> </a> <a href="https://twitter.com/intent/tweet?text=/cs224n-1/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=/cs224n-1/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> --> <div style="clear:both"></div> </div> </div> </div> <section id="disqus_thread" class="animated fadeInUp"></section><!-- /#disqus_thread --> </header> <!-- JS --> <script src="/assets/js/jquery-1.12.0.min.js"></script> <script src="/assets/js/jquery.dlmenu.min.js"></script> <script src="/assets/js/jquery.goup.min.js"></script> <script src="/assets/js/jquery.magnific-popup.min.js"></script> <script src="/assets/js/jquery.fitvid.min.js"></script> <script src="/assets/js/scripts.js"></script> <!-- MathJax --> <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script> </body> </html>
