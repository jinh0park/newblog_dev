<!DOCTYPE html> <!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--> <!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--> <!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--> <!--[if gt IE 8]><!--> <html class="no-js"><!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <title>강의 요약 - CS224n: Natural Language Processing with Deep Learning (3) &#8211; JINH-ZERO-PARK</title> <meta name="description" content="Welcome."> <meta name="keywords" content=""> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="/assets/img/logo.png"> <meta name="twitter:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning (3)"> <meta name="twitter:description" content="I. Contents Neural Dependency Parsing Neural Dependency Parsing A bit more about neural Networks Language Modeling and RNNs Language Modeling N-grams Language Models Neural Language Models Evaluating Language Models LSTM : Long Short-Term Memory RNNs Problems with Vanishing and Exploding Gradients LSTMs More about vanishing/exploding gradient Problem Bidirectional and Multi-layer RNNs: Motivation Summary"> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning (3)"> <meta property="og:description" content="I. Contents Neural Dependency Parsing Neural Dependency Parsing A bit more about neural Networks Language Modeling and RNNs Language Modeling N-grams Language Models Neural Language Models Evaluating Language Models LSTM : Long Short-Term Memory RNNs Problems with Vanishing and Exploding Gradients LSTMs More about vanishing/exploding gradient Problem Bidirectional and Multi-layer RNNs: Motivation Summary"> <meta property="og:url" content="/cs224n-3/"> <meta property="og:site_name" content="JINH-ZERO-PARK"> <meta property="og:image" content="/assets/img/logo.png"> <link rel="canonical" href="/cs224n-3/"> <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="JINH-ZERO-PARK Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="/assets/css/main.css"> <!-- JS --> <script src="/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="/favicon.png" /> <link rel="shortcut icon" href="/favicon.ico" /> <!-- Background Image --> <style type="text/css">body {background-image:url(/assets/img/placeholder-big.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="/assets/img/logo.png" alt="JINH-ZERO-PARK photo" class="author-photo"> <h4>JINH-ZERO-PARK</h4> <p>Welcome.</p> </li> <li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="mailto:jinh0park@naver.com" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-envelope-square"></i> Email</a> </li> <li> <a href="http://instagram.com/jinh0park" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-instagram"></i> Instagram</a> </li> <li> <a href="http://github.com/jinh0park" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> </ul><!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="/posts/">All Posts</a></li> <li><a href="/tags/">All Tags</a></li> </ul> </li> <li><a href="/projects/" >Projects</a></li> </ul><!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>강의 요약 - CS224n: Natural Language Processing with Deep Learning (3)</h1> <h4>30 Aug 2022</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~5 minutes </p><!-- /.entry-reading-time --> <a class="btn zoombtn" href="/posts/"> <i class="fa fa-chevron-left"></i> </a> </div> <h1 id="i-contents">I. Contents</h1> <ul> <li>Neural Dependency Parsing <ul> <li>Neural Dependency Parsing</li> <li>A bit more about neural Networks</li> </ul> </li> <li>Language Modeling and RNNs <ul> <li>Language Modeling</li> <li>N-grams Language Models</li> <li>Neural Language Models</li> <li>Evaluating Language Models</li> </ul> </li> <li>LSTM : Long Short-Term Memory RNNs <ul> <li>Problems with Vanishing and Exploding Gradients</li> <li>LSTMs</li> <li>More about vanishing/exploding gradient Problem</li> <li>Bidirectional and Multi-layer RNNs: Motivation</li> </ul> </li> <li>Summary</li> </ul> <h1 id="ii-neural-dependency-parsing">II. Neural Dependency Parsing</h1> <h2 id="1-neural-dependency-parsing">1. Neural Dependency Parsing</h2> <p>Deep learning classifiers are non-linear classifiers (cf. Traditional ML classifiers only give linear decision boundaries)</p> <p><a href="https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a> Chen and Manning, 2014</p> <h2 id="2-a-bit-more-about-neural-networks">2. A bit more about neural networks</h2> <ul> <li>Regularization</li> <li>Dropout</li> <li>Vectorization</li> <li>Non-linearities</li> <li>Parameter initialization</li> <li>Optimizers</li> <li>Learning rates</li> </ul> <h1 id="iii-language-modeling-and-rnns">III. Language modeling and RNNs</h1> <h2 id="1-language-modeling">1. Language Modeling</h2> <p><strong>Language Modeling</strong> is the task of predicting what word comes next.</p> <p>More formally: given a sequence of words \(x^{(1)},x^{(2)},\cdots,x^{(t)}\), compute the probability distribution of the next word \(x^{(t+1)}\)</p> <p>\[P(x^{(t+1)}\mid x^{(t)},\cdots x^{(1)})\]</p> <p>where \(x^{(t+1)}\) can be any word in the vocabulary \(V = {w_1, \cdots , w_{\mid V\mid }}\)</p> <p>주어진 단어들의 sequence가 있을 때, 그 다음에 올 단어의 확률분포를 구하는 것을 Language modeling이라고 한다.</p> <p>각 sequence step마다 “단어가 다음에 올 확률”을 곱하면 전체 텍스트의 확률 분포가 되며, 식은 아래와 같다.</p> <p>\[P(x^{(1)},\cdots ,x^{(T)})=P(x^{(1)})\times P(x^{(2)\mid x^(1)}) \times \cdots \ = \prod_{t=1}^T P(x^{(t)}\mid x^{(t-1)},\cdots ,x^{(1)}) \]</p> <h2 id="2-n-gram-language-models">2. N-gram Language Models</h2> <p>Idea : Collect statistics about how frequent different n-grams are and use these to predict next word.</p> <p>First we make a <strong>Markov assumption</strong> : \(x^{(t+1)}\) depends only on the preceding \(n-1\) words</p> <p>\[P(x^{(t+1)}\mid x^{(t)},\cdots x^{(1)}) = P(x^{(t+1)}\mid x^{(t)},\cdots x^{(t-n+2)}) \ = \frac {P(x^{(t+1)},x^{(t)},\cdots x^{(t-n+2)})} {P(x^{(t)},\cdots x^{(t-n+2)})} \]</p> <p>\[\approx \frac {count(x^{(t+1)},x^{(t)},\cdots x^{(t-n+2)})} {count(x^{(t)},\cdots x^{(t-n+2)})}\]</p> <h3 id="problems-of-n-gram">Problems of N-gram</h3> <ul> <li>Sparsity Problems <ul> <li>Problem 1 : 위 식에서 분자 부분의 count가 0이라면, 해당 단어의 확률이 0으로 고정됨 <br /> Solution : Add small \(\delta\) to the count for every \(w \in V\)</li> <li>Problem 1 : 위 식에서 분모 부분의 count가 0이라면, 그 다음 단어의 확률을 정의할 수 없음 <br /> Solution : 마지막 단어 하나 생략하고 찾기</li> </ul> </li> <li>Storage Problems <ul> <li>Need to store count for all n-grams you saw in the corpus.</li> </ul> </li> </ul> <p>Results : Surprisingly grammatical!</p> <p>…but <strong>incoherent</strong>. We need to consider more than three words at a time if we want to model language well. But increasing n worsens <strong>sparsity problem</strong>, and increase model size.</p> <h2 id="3-neural-language-models">3. Neural Language Models</h2> <h3 id="1-a-fixed-window-neural-language-model">(1) A fixed-window neural Language Model</h3> <p><a href="https://jmlr.org/papers/volume3/tmp/bengio03a.pdf">A Neural Probabilistic Language Model</a>, Y.Bengio, et al. (2000/2003)</p> <p><img src="/img/posts/cs224n/8.png" alt="" /></p> <h4 id="improvements-over-n-gram-lm">Improvements over n-gram LM</h4> <ul> <li>No sparsity Problem</li> <li>Don’t need to store all observed n-grams</li> </ul> <h4 id="remaining-problems">Remaining Problems</h4> <ul> <li>Fixed window is too small</li> <li>Enlarging window enlarges \(W\) ☞ Window can never be large enough!</li> <li>No symmetry in how the inputs are processed</li> </ul> <p>☞ We need a neural architecture that can process any length input!</p> <h3 id="2-recurrent-neural-networks">(2) Recurrent Neural Networks</h3> <p><strong>Core idea</strong> : Apply the same weights \(W\) repeatedly!</p> <p><img src="/img/posts/cs224n/9.png" alt="" /></p> <h4 id="rnn-advantages">RNN Advantages</h4> <ul> <li>Can process <strong>any length</strong> input</li> <li>Computation for step \(t\) can (in theory[<em>due to gradient vanishing problem, “in theory”</em>]) use information from many steps back</li> <li><strong>Model size doesn’t increase</strong> for longer input context</li> <li>Same weights applied on every timestep, so there is <strong>symmetry</strong> in how inputs are processed.</li> </ul> <h4 id="rnn-disadvantages">RNN Disadvantages</h4> <ul> <li>Recurrent computation is slow (it runs in the for loop, can’t be computed parallelly)</li> <li>In practice, difficult to access information from many steps back</li> </ul> <h4 id="training-an-rnn-language-models">Training an RNN Language Models</h4> <p>\[J^{(t)}(\theta)=CE(y^{(t)}, \hat y^{(t)})=-\sum_{w\in V}y_w^{(t)}=-log\hat y^{(t)}<em>{x</em>{t+1}}\]</p> <p>\[J(\theta)=\frac {1}{T} \sum^T_{t=1}J^{(t)}(\theta)\]</p> <h2 id="4-evaluating-language-models">4. Evaluating Language Models</h2> <p>The standard evaluation metric for LM is <strong>perplexity</strong></p> <p>\[perplexity = \prod_{t=1}^T (\frac {1}{P_{LM}(x^{(t+1)}\mid x^{(t)},\cdots ,x^{(1)})})^{1/T} \ = exp(J(\theta))\]</p> <p><strong>Lower</strong> perplexity is better!</p> <p>probability of corpus의 기하평균의 역, 모든 단어를 정확히 맞춘다면 \(perplexity = 1\)</p> <h3 id="why-should-we-care-about-language-modeling">Why should we care about Language Modeling?</h3> <ul> <li>Language Modeling is a <strong>benchmark task</strong> that helps us <strong>measure our progress</strong> on understanding language</li> <li>Language Modeling is a <strong>subcomponent</strong> of many NLP tasks</li> </ul> <h1 id="iv-lstm--long-short-term-memory-rnns">IV. LSTM : Long Short-Term Memory RNNs</h1> <h2 id="1-problems-with-vanishing-and-exploding-gradients">1. Problems with Vanishing and Exploding Gradients</h2> <h3 id="vanishing-gradients">Vanishing gradients</h3> <p><img src="/img/posts/cs224n/10.png" alt="" /></p> <h3 id="exploding-gradients">Exploding gradients</h3> <p>Exploding gradients can solve by simple methods such as <strong>gradient clipping</strong> .</p> <p>How about a RNN with separate memory to fix the <strong>vanishing</strong> gradient problem? ☞ <strong>LSTMs</strong></p> <h2 id="2-lstms">2. LSTMs</h2> <p><a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter and Schmidhuber</a>(1997)</p> <p><a href="https://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf">Gers et al</a>(2000) -&gt; Crucial part of the modern LSTM is here!</p> <p>On step \(t\), there is a hidden state \(h^{(t)}\) and a cell state \(c^{(t)}\)</p> <ul> <li>Both are vectors length \(n\)</li> <li>The cell stores <strong>long-term information</strong></li> <li>The LSTM can read, erase, and write information from the cell <ul> <li>the cell becomes conceptually rather like RAM in a computer</li> </ul> </li> </ul> <p><img src="/img/posts/cs224n/11.png" alt="" /></p> <p><img src="/img/posts/cs224n/12.png" alt="" /></p> <p>LSTM이 Gradient vanishing을 해결할 수 있는 핵심 구조는 cell state이다. cell state는 곱연산이 아닌 <strong>합연산</strong>을 통해 다음 cell로 전해지므로, step을 오래 거치더라도 vanishing이 발생하지 않는다.</p> <p>cell state에 long term memory가 저장되므로, hidden state를 계산할 때 output gate를 통해 long term memory에 저장된 정보를 얼마나 사용할지 결정할 수 있다.</p> <h3 id="3-more-about-vanishingexploding-gradient-problem">3. More about vanishing/exploding gradient Problem</h3> <h4 id="is-vanishingexploding-gradient-just-a-rnn-problem">Is vanishing/exploding gradient just a RNN problem?</h4> <p>No! It can be a problem for all neural architectures, especially very deep ones</p> <p>Solution : Add more direct connections (e.g. ResNet, DenseNet, HighwayNet, and etc.)</p> <h3 id="4-bidirectional-and-multi-layer-rnns-motivation">4. Bidirectional and Multi-layer RNNs: Motivation</h3> <h4 id="1-bidirectional-rnns">(1) Bidirectional RNNs</h4> <p>문장 구조상, 뒤에 있는 단어들 까지 보아야 단어의 의미를 파악할 수 있는 경우가 있다. (e.g. <em>the movie was “teriibly” exciting!</em>, <em>terribly</em>가 긍정적인 의미로 사용되었음을 알기 위해서는 뒤의 <em>exciting</em>도 보아야 한다.)</p> <p><img src="/img/posts/cs224n/13.png" alt="" /></p> <p>\(h^{(t)}\) has the dimension of \(2d\) (\(d\) is the hidden size of FW or BW)</p> <p>Note: Bidirectional RNNs are only applicable if you have access to the <strong>entire input sequence</strong> ☞ Not applicable to LM!</p> <h4 id="2-multi-layer-rnns">(2) Multi-layer RNNs</h4> <p><img src="/img/posts/cs224n/14.png" alt="" /></p> <h1 id="v-summary">V. Summary</h1> <p>Language Modeling은 자연어처리에서 benchmark test &amp; subcomponent 역할을 하는 task이다. 딥러닝 이전에 N-grams LM이 존재하였으며, RNN을 도입하면서 성능이 비약적으로 상승하였다.</p> <p>한편, RNN의 특성상 gradient vanishing(exploding) 문제가 발생하는데, 이를 해결하기 위해 RNNs중 하나로서 LSTMs이 도입되었다. LSTM의 핵심은 long term memory를 저장하는 cell state로, 합연산을 통해 값이 전달되므로 vanishing이 현저하게 줄어든다.</p> <p>RNNs의 성능을 더욱 향상시키기 위한 시도로 Bidirectional, Multi-layer RNNs 등이 있으며, 오늘날 가장 좋은 성능을 보이는 모델 중 하나인 BERT와 같은 Transformer-based network에서도 이러한 구조들을 채택하고 있다.</p> <div class="entry-meta"> <br> <hr> <span class="entry-tags"></span> <!-- <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=/cs224n-3/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Share</span> </a> <a href="https://twitter.com/intent/tweet?text=/cs224n-3/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=/cs224n-3/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> --> <div style="clear:both"></div> </div> </div> </div> <section id="disqus_thread" class="animated fadeInUp"></section><!-- /#disqus_thread --> </header> <!-- JS --> <script src="/assets/js/jquery-1.12.0.min.js"></script> <script src="/assets/js/jquery.dlmenu.min.js"></script> <script src="/assets/js/jquery.goup.min.js"></script> <script src="/assets/js/jquery.magnific-popup.min.js"></script> <script src="/assets/js/jquery.fitvid.min.js"></script> <script src="/assets/js/scripts.js"></script> <script type="text/javascript"> var disqus_shortname = 'jinh0park'; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
